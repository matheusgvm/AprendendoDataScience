{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5467766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [cachorro, gato, tartaruga, papagaio]\n",
      "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'animal_component']\n",
      "[('tartaruga', 'ANIMAL'), ('cachorro', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "#Personalizando os componentes de um pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "animals = [\"cachorro\", \"gato\", \"tartaruga\", \"papagaio\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Defininindo o componente customizado\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Aplicando o matcher ao doc\n",
    "    matches = matcher(doc)\n",
    "    # Criando uma partição para cada correspondência e atribuir=ndo o rótulo \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescrevendo doc.ents com as correspondências \n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Adicionando o componente no final do fluxo de processamento \n",
    "nlp.add_pipe(\"animal_component\", last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Processando o texto e imprimir o texto e rótulo de doc.ents\n",
    "doc = nlp(\"Eu tenho uma tartaruga e um cachorro\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebef8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionando atributos\n",
    "\n",
    "\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "#Span.set_extension(\"has_color\", default=False)\n",
    "\n",
    "doc = nlp(\"O céu é azul.\")\n",
    "\n",
    "# Acessando o novo atributo criado\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "# Para o Span tem que usar a extensão de propriedade com uma função getter\n",
    "def get_has_color(span):\n",
    "    colors = [\"vermelho\", \"amarelo\", \"azul\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Definindo a extensão com o parametro getter sendo a função definida\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd31500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - azul\n",
      "False - nuvem\n"
     ]
    }
   ],
   "source": [
    "#Adicionando métodos\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Definindo método com seus parâmetros\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Definindo a extensão do documento com o parâmetro nomeado method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"O céu é azul.\")\n",
    "print(doc._.has_token(\"azul\"), \"- azul\")\n",
    "print(doc._.has_token(\"nuvem\"), \"- nuvem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6440ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aumentando escala e desempenho\n",
    "\n",
    "#Jeito mais eficiente de criar vários Docs de uma vez\n",
    "#É interessante criar Docs diferentes para textos diferentes, como por exmplo, um conjunto de tweets sobre um determinado assunto\n",
    "\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736b02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isso é um texto 15\n",
      "Isso é um outro texto 16\n"
     ]
    }
   ],
   "source": [
    "#Ativando as duplas para passar metadados adicionais\n",
    "#Dá para criar atributos personalizados com esses metadados\n",
    "\n",
    "data = [\n",
    "    (\"Isso é um texto\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"Isso é um outro texto\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a624dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método usado para criar um Doc passando apenas pelo processo de tokenização\n",
    "\n",
    "doc = nlp.make_doc(\"Olá!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é possível desabilitar temporariamente componentes do fluxo de processamento\n",
    "\n",
    "# Desabilitando o tagueador tagger e o analisador parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
